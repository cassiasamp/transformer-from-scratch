{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb36b237",
   "metadata": {},
   "source": [
    "## scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89df1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Implementing the Transformer Encoder from Scratch in TensorFlow and Keras \n",
    "# by Stefania Cristina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7373598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import matmul, math, cast, float32\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04476d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    " \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    " \n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    " \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2482214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.78870237 0.7197107  0.5789627  ... 0.30551153 0.5200055  0.8495083 ]\n",
      "  [0.78455234 0.7103207  0.56919247 ... 0.31573522 0.50821453 0.8410804 ]\n",
      "  [0.7859878  0.7136792  0.5775507  ... 0.30687004 0.5122791  0.8534212 ]\n",
      "  [0.79245305 0.7095282  0.57745343 ... 0.30003113 0.50347376 0.84879875]\n",
      "  [0.78900033 0.719824   0.5773204  ... 0.30418324 0.51386553 0.85077745]]\n",
      "\n",
      " [[0.58441305 0.575968   0.4403424  ... 0.6077771  0.59896576 0.43121126]\n",
      "  [0.57320523 0.5799672  0.42515942 ... 0.6197639  0.5942051  0.43821064]\n",
      "  [0.5729611  0.5759233  0.4239864  ... 0.62295467 0.5912265  0.4352289 ]\n",
      "  [0.5482842  0.60377926 0.3993437  ... 0.61313146 0.5884461  0.46253645]\n",
      "  [0.58554167 0.5784456  0.44175237 ... 0.6011471  0.59930706 0.43158135]]\n",
      "\n",
      " [[0.441461   0.6668739  0.8001705  ... 0.60246295 0.3074212  0.63430285]\n",
      "  [0.45316544 0.659326   0.79943967 ... 0.6030458  0.2999731  0.6446727 ]\n",
      "  [0.43863583 0.65360427 0.7960007  ... 0.6170958  0.30268198 0.65139717]\n",
      "  [0.4390714  0.65678376 0.79959303 ... 0.6160094  0.2989295  0.652111  ]\n",
      "  [0.4405422  0.6611987  0.79968774 ... 0.6124517  0.2982677  0.6464871 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.57520837 0.44318432 0.33226144 ... 0.59006554 0.6826786  0.3196105 ]\n",
      "  [0.56347334 0.45321792 0.3409314  ... 0.5971396  0.6720792  0.31552535]\n",
      "  [0.5804655  0.4599235  0.321341   ... 0.591601   0.68029106 0.31478494]\n",
      "  [0.5674422  0.45985675 0.33921978 ... 0.5931377  0.66994196 0.31665033]\n",
      "  [0.56355643 0.46458113 0.35605013 ... 0.58155394 0.64948124 0.33431095]]\n",
      "\n",
      " [[0.33313397 0.3568598  0.73134655 ... 0.56454366 0.3703025  0.547807  ]\n",
      "  [0.32874936 0.35854188 0.7265877  ... 0.5606442  0.37067345 0.5441824 ]\n",
      "  [0.3330466  0.36062908 0.7335144  ... 0.55579793 0.35815877 0.55305564]\n",
      "  [0.33124244 0.37660348 0.71119875 ... 0.5709441  0.34459567 0.5235402 ]\n",
      "  [0.3335734  0.36402708 0.7287639  ... 0.55793047 0.35839552 0.54700243]]\n",
      "\n",
      " [[0.51720476 0.3164676  0.45969263 ... 0.16637075 0.6773086  0.5410262 ]\n",
      "  [0.49696264 0.28853568 0.43471536 ... 0.18837154 0.6488074  0.5277577 ]\n",
      "  [0.4978932  0.30227464 0.43326214 ... 0.18551677 0.6827034  0.5008767 ]\n",
      "  [0.51919895 0.29019383 0.43061563 ... 0.17920275 0.6734896  0.50400394]\n",
      "  [0.49861765 0.30809295 0.44727874 ... 0.18003431 0.6714724  0.52781975]]], shape=(64, 5, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "batch_size = 64  # Batch size from the training process\n",
    "\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b166d",
   "metadata": {},
   "source": [
    "## multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6298d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8806421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92cfb127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 6.71806708e-02  1.93809718e-02 -3.66341263e-01 ...  3.98182333e-01\n",
      "   -3.96931946e-01  1.31076112e-01]\n",
      "  [ 6.87384531e-02  1.87794100e-02 -3.68321449e-01 ...  4.00790304e-01\n",
      "   -4.00350243e-01  1.30587667e-01]\n",
      "  [ 7.09506199e-02  1.94051936e-02 -3.65669250e-01 ...  3.99501950e-01\n",
      "   -3.98961604e-01  1.32188201e-01]\n",
      "  [ 7.36817047e-02  2.26091556e-02 -3.65710586e-01 ...  4.03091371e-01\n",
      "   -4.02278811e-01  1.32276833e-01]\n",
      "  [ 6.65156469e-02  2.34209374e-02 -3.60081851e-01 ...  4.01341826e-01\n",
      "   -3.97625417e-01  1.36518389e-01]]\n",
      "\n",
      " [[ 1.91446513e-01  2.49095373e-02 -2.29071140e-01 ...  4.73585933e-01\n",
      "   -3.84122312e-01  2.77026981e-01]\n",
      "  [ 1.93208367e-01  2.61090584e-02 -2.31012493e-01 ...  4.76545960e-01\n",
      "   -3.81521463e-01  2.78961509e-01]\n",
      "  [ 1.92307055e-01  2.39113905e-02 -2.31784299e-01 ...  4.75757331e-01\n",
      "   -3.83505672e-01  2.76860595e-01]\n",
      "  [ 1.91757426e-01  2.61494871e-02 -2.33344749e-01 ...  4.77425247e-01\n",
      "   -3.81760120e-01  2.81350285e-01]\n",
      "  [ 1.93043664e-01  2.88456008e-02 -2.30410039e-01 ...  4.74502355e-01\n",
      "   -3.83505166e-01  2.81879216e-01]]\n",
      "\n",
      " [[ 2.68446267e-01  8.27162061e-03 -3.14260006e-01 ...  4.90311891e-01\n",
      "   -4.39353228e-01  2.55383283e-01]\n",
      "  [ 2.67286032e-01  7.32787978e-03 -3.12636048e-01 ...  4.89525616e-01\n",
      "   -4.38304096e-01  2.53692001e-01]\n",
      "  [ 2.67697304e-01  5.32214623e-03 -3.11135769e-01 ...  4.88638282e-01\n",
      "   -4.38638777e-01  2.52866477e-01]\n",
      "  [ 2.68804818e-01  8.32299888e-03 -3.12341750e-01 ...  4.89837229e-01\n",
      "   -4.39274490e-01  2.52499044e-01]\n",
      "  [ 2.68361479e-01  9.07289237e-03 -3.13505977e-01 ...  4.90930766e-01\n",
      "   -4.36114013e-01  2.53342122e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.67225415e-01  1.45808801e-01 -2.94105977e-01 ...  4.66560334e-01\n",
      "   -3.87284487e-01  2.96403587e-01]\n",
      "  [ 2.63555557e-01  1.42697379e-01 -2.96131015e-01 ...  4.69966382e-01\n",
      "   -3.89486939e-01  3.00930798e-01]\n",
      "  [ 2.61897892e-01  1.41046301e-01 -2.96322167e-01 ...  4.70326662e-01\n",
      "   -3.89408916e-01  3.00320536e-01]\n",
      "  [ 2.64287710e-01  1.43847883e-01 -2.92930067e-01 ...  4.69280273e-01\n",
      "   -3.86610299e-01  2.99252927e-01]\n",
      "  [ 2.64009863e-01  1.43133327e-01 -2.94040680e-01 ...  4.67105061e-01\n",
      "   -3.88451755e-01  3.00325632e-01]]\n",
      "\n",
      " [[ 1.37186766e-01  1.41190074e-03 -2.53547311e-01 ...  4.57561225e-01\n",
      "   -3.05853218e-01  2.25152999e-01]\n",
      "  [ 1.37321174e-01 -3.90930130e-04 -2.57342637e-01 ...  4.58800644e-01\n",
      "   -3.09640318e-01  2.21815720e-01]\n",
      "  [ 1.35288894e-01 -3.81901424e-04 -2.56588966e-01 ...  4.61180031e-01\n",
      "   -3.08874279e-01  2.23772004e-01]\n",
      "  [ 1.37460187e-01 -9.49443434e-04 -2.58187115e-01 ...  4.58026826e-01\n",
      "   -3.08262736e-01  2.23714426e-01]\n",
      "  [ 1.39315143e-01  5.23559283e-04 -2.56073475e-01 ...  4.60050374e-01\n",
      "   -3.09870690e-01  2.23382786e-01]]\n",
      "\n",
      " [[ 2.29963943e-01 -1.24653362e-01 -2.54559934e-01 ...  4.00498629e-01\n",
      "   -3.61867636e-01  2.34636977e-01]\n",
      "  [ 2.20844686e-01 -1.23950511e-01 -2.53565937e-01 ...  3.98922652e-01\n",
      "   -3.61655325e-01  2.34609187e-01]\n",
      "  [ 2.23024726e-01 -1.21926382e-01 -2.53605813e-01 ...  3.98701906e-01\n",
      "   -3.61743808e-01  2.32013613e-01]\n",
      "  [ 2.20007002e-01 -1.21187665e-01 -2.53623664e-01 ...  4.00495231e-01\n",
      "   -3.59198302e-01  2.32539102e-01]\n",
      "  [ 2.20414534e-01 -1.22283340e-01 -2.53046364e-01 ...  4.00518000e-01\n",
      "   -3.56370002e-01  2.32957348e-01]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    " \n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    " \n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    " \n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "print(multihead_attention(queries, keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da387e",
   "metadata": {},
   "source": [
    "# position embedding with fixed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66d7463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cee3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/\n",
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    " \n",
    " \n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaccf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cffbf72",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db1c8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10864c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from keras.layers import MultiHeadAttention\n",
    "# from positional_encoding import PositionEmbeddingFixedWeights\n",
    "from keras_nlp.layers import PositionEmbedding, TokenAndPositionEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7c6a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        #self.pos_encoding = PositionEmbedding(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "474c3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95bb5647",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Frequency of dropping the input units in the dropout layers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom((batch_size, input_seq_length))\n\u001b[0;32m---> 15\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_vocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoder(input_seq, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn [46], line 65\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Encoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mPositionEmbeddingFixedWeights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#self.pos_encoding = PositionEmbedding(sequence_length, vocab_size, d_model)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m Dropout(rate)\n",
      "Cell \u001b[0;32mIn [43], line 4\u001b[0m, in \u001b[0;36mPositionEmbeddingFixedWeights.__init__\u001b[0;34m(self, sequence_length, vocab_size, output_dim, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequence_length, vocab_size, output_dim, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(PositionEmbeddingFixedWeights, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m----> 4\u001b[0m     word_embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_position_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m      5\u001b[0m     position_embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_position_encoding(sequence_length, output_dim)                                          \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_layer \u001b[38;5;241m=\u001b[39m Embedding(\n\u001b[1;32m      7\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39moutput_dim,\n\u001b[1;32m      8\u001b[0m         weights\u001b[38;5;241m=\u001b[39m[word_embedding_matrix],\n\u001b[1;32m      9\u001b[0m         trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     )\n",
      "Cell \u001b[0;32mIn [43], line 18\u001b[0m, in \u001b[0;36mPositionEmbeddingFixedWeights.get_position_encoding\u001b[0;34m(self, seq_len, d, n)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_position_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m, seq_len, d, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((seq_len, d))\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mint\u001b[39m(d\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706912d1",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eed52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
