{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b90a723",
   "metadata": {},
   "source": [
    "## scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e685e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Implementing the Transformer Encoder from Scratch in TensorFlow and Keras \n",
    "# by Stefania Cristina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45759d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import matmul, math, cast, float32\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10b55a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    " \n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    " \n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    " \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65e48e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring\n",
    "c, s = [[1, 0], [0, 1]], [[1, 1], [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76d60f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 0],\n",
       "       [1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul(c, s, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f6f120c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 1],\n",
       "       [0, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul(c, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5dc4fa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.5065605  0.28694072]\n",
      "  [0.5238924  0.28643516]]], shape=(1, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "input_seq_length = 2  # Maximum length of the input sequence\n",
    "d_k = 2  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 2  # Dimensionality of the linearly projected values\n",
    "batch_size = 1  # Batch size from the training process\n",
    "\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "attention = DotProductAttention()\n",
    "print(attention(queries, keys, values, d_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aec1bf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2, 2), (1, 2, 2), (1, 2, 2))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47039b67",
   "metadata": {},
   "source": [
    "## multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bda7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2171148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507b5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.37542415  0.16370712  0.2722     ...  0.01764775 -0.32050243\n",
      "   -0.16811453]\n",
      "  [ 0.3759839   0.161696    0.26840478 ...  0.01800927 -0.31681502\n",
      "   -0.16821906]\n",
      "  [ 0.3719455   0.16542085  0.2711065  ...  0.02029621 -0.31939974\n",
      "   -0.16509406]\n",
      "  [ 0.3771822   0.16032702  0.26628378 ...  0.02103805 -0.31612858\n",
      "   -0.1685995 ]\n",
      "  [ 0.37612554  0.16318063  0.27191094 ...  0.01662399 -0.31985164\n",
      "   -0.16343614]]\n",
      "\n",
      " [[ 0.3225099   0.17144611  0.24480078 ...  0.0222888  -0.3693074\n",
      "   -0.2831262 ]\n",
      "  [ 0.3200459   0.17515285  0.24452926 ...  0.02390499 -0.37186792\n",
      "   -0.28548586]\n",
      "  [ 0.318898    0.17492948  0.24313046 ...  0.02379028 -0.37319547\n",
      "   -0.28479773]\n",
      "  [ 0.3227564   0.17036484  0.24227251 ...  0.02468883 -0.37046862\n",
      "   -0.28382224]\n",
      "  [ 0.31911483  0.17439531  0.2440254  ...  0.0183255  -0.37463886\n",
      "   -0.2840677 ]]\n",
      "\n",
      " [[ 0.24696773  0.08486743  0.1325606  ...  0.12664165 -0.39490405\n",
      "   -0.28588724]\n",
      "  [ 0.24686381  0.08382798  0.13392176 ...  0.12675825 -0.39631346\n",
      "   -0.28344434]\n",
      "  [ 0.24621986  0.08326299  0.13457905 ...  0.12704916 -0.39922836\n",
      "   -0.2859047 ]\n",
      "  [ 0.24860416  0.08495518  0.1341257  ...  0.12551865 -0.39689907\n",
      "   -0.28527963]\n",
      "  [ 0.24753568  0.08385514  0.13510664 ...  0.12844834 -0.39816526\n",
      "   -0.28369224]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.12935281  0.2070438   0.12790252 ...  0.1516179  -0.22207583\n",
      "   -0.23845625]\n",
      "  [ 0.13172334  0.2067788   0.12652868 ...  0.1505372  -0.22361635\n",
      "   -0.2409604 ]\n",
      "  [ 0.12893198  0.20606117  0.12536703 ...  0.15106976 -0.22102712\n",
      "   -0.23771542]\n",
      "  [ 0.13297673  0.20416918  0.12491594 ...  0.1531305  -0.22258684\n",
      "   -0.23842545]\n",
      "  [ 0.13140078  0.20809725  0.12599574 ...  0.15012456 -0.2230901\n",
      "   -0.2394313 ]]\n",
      "\n",
      " [[ 0.37907586  0.17450064  0.1100207  ...  0.13707858 -0.32551417\n",
      "   -0.23559524]\n",
      "  [ 0.37438458  0.17402664  0.11491921 ...  0.13769019 -0.3222245\n",
      "   -0.24041596]\n",
      "  [ 0.37610397  0.17450978  0.11515889 ...  0.13557032 -0.32536215\n",
      "   -0.23912384]\n",
      "  [ 0.3813851   0.17569053  0.11359107 ...  0.13687356 -0.3232386\n",
      "   -0.2348911 ]\n",
      "  [ 0.3765208   0.17727551  0.11307576 ...  0.13711534 -0.32384413\n",
      "   -0.23918779]]\n",
      "\n",
      " [[ 0.26626775  0.16281927  0.1604353  ...  0.07319651 -0.29426616\n",
      "   -0.18799286]\n",
      "  [ 0.2634326   0.16675113  0.15982404 ...  0.07167456 -0.29335785\n",
      "   -0.18723287]\n",
      "  [ 0.26671773  0.16582268  0.16266339 ...  0.0678876  -0.29248932\n",
      "   -0.18606086]\n",
      "  [ 0.26455867  0.1645801   0.16020918 ...  0.07301368 -0.29266733\n",
      "   -0.18646953]\n",
      "  [ 0.2714038   0.1644591   0.16071412 ...  0.06844024 -0.29304728\n",
      "   -0.18717155]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    " \n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    " \n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    " \n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "print(multihead_attention(queries, keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba271e",
   "metadata": {},
   "source": [
    "# position embedding with fixed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cfc4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf \n",
    "import numpy as np\n",
    "#from tf.keras.layers import Embedding\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab24efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor, string\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
    "from tensorflow.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3313fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/\n",
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    " \n",
    " \n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "969ae1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\n",
      "Vectorized words:  tf.Tensor(\n",
      "[[5 6 7 2 0]\n",
      " [3 4 2 0 0]], shape=(2, 5), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 14:40:17.890303: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "output_sequence_length = 5\n",
    "vocab_size = 10\n",
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "sentence_data = Dataset.from_tensor_slices(sentences)\n",
    "# Create the TextVectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "                  output_sequence_length=output_sequence_length,\n",
    "                  max_tokens=vocab_size)\n",
    "# Train the layer to create a dictionary\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "# Convert all sentences to tensors\n",
    "word_tensors = convert_to_tensor(sentences, dtype=tf.string)\n",
    "# Use the word tensors to get vectorized phrases\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
    "print(\"Vectorized words: \", vectorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5cf829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.00147907  0.04239791 -0.02854638  0.04900427 -0.03686076\n",
      "    0.0365614 ]\n",
      "  [-0.04042323 -0.04642573 -0.00339592 -0.03967012 -0.00826682\n",
      "   -0.01464304]\n",
      "  [-0.01009899 -0.02796451  0.03379272  0.03272815  0.03033458\n",
      "   -0.03182179]\n",
      "  [-0.04701636 -0.04893167  0.02985634 -0.04636011 -0.01471276\n",
      "    0.02737038]\n",
      "  [ 0.01645939  0.01950083  0.04410869  0.02758285  0.01806964\n",
      "   -0.0495386 ]]\n",
      "\n",
      " [[-0.01020547 -0.04073662  0.02055651 -0.04205241  0.03845442\n",
      "    0.00208529]\n",
      "  [-0.00175231  0.02371711  0.03997305 -0.00950316 -0.02988769\n",
      "   -0.03083377]\n",
      "  [-0.04701636 -0.04893167  0.02985634 -0.04636011 -0.01471276\n",
      "    0.02737038]\n",
      "  [ 0.01645939  0.01950083  0.04410869  0.02758285  0.01806964\n",
      "   -0.0495386 ]\n",
      "  [ 0.01645939  0.01950083  0.04410869  0.02758285  0.01806964\n",
      "   -0.0495386 ]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output_length = 6\n",
    "word_embedding_layer = Embedding(vocab_size, output_length)\n",
    "embedded_words = word_embedding_layer(vectorized_words)\n",
    "print(embedded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070dd0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.01392437  0.04893779  0.03891344  0.02957899  0.04270632  0.01377598]\n",
      " [-0.0304196   0.04172793 -0.00196735  0.00806618  0.04934986  0.03682312]\n",
      " [ 0.01089218 -0.00961496 -0.03673936  0.02197268 -0.0171245   0.02153582]\n",
      " [-0.04367065  0.03288157  0.01019937 -0.02111708  0.01883158  0.00485639]\n",
      " [-0.04088131 -0.04692424 -0.00144348  0.03051959 -0.03888492 -0.02046024]], shape=(5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "position_embedding_layer = Embedding(output_sequence_length, output_length)\n",
    "position_indices = tf.range(output_sequence_length)\n",
    "embedded_indices = position_embedding_layer(position_indices)\n",
    "print(embedded_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e1c093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from my_embedded_layer:  tf.Tensor(\n",
      "[[[-0.9589243   1.2836622   0.23000172  1.9731903   0.01077196\n",
      "    1.9999421 ]\n",
      "  [ 0.56205547  1.5004725   0.3213085   1.9603932   0.01508068\n",
      "    1.9999142 ]\n",
      "  [ 1.566284    0.3377554   0.41192317  1.9433732   0.01938933\n",
      "    1.999877  ]\n",
      "  [ 1.0504174  -1.4061394   0.2314966   1.9860148   0.01077211\n",
      "    1.9999698 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]\n",
      "\n",
      " [[ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [ 0.08466846 -0.11334133  0.23099795  1.9817369   0.01077207\n",
      "    1.9999605 ]\n",
      "  [ 1.8185948  -0.8322937   0.185397    1.9913884   0.00861771\n",
      "    1.9999814 ]\n",
      "  [ 0.14112     0.0100075   0.1387981   1.9903207   0.00646326\n",
      "    1.9999791 ]\n",
      "  [-0.7568025   0.3463564   0.18459873  1.982814    0.00861763\n",
      "    1.9999628 ]]], shape=(2, 5, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attnisallyouneed_embedding = PositionEmbeddingFixedWeights(output_sequence_length,\n",
    "                                            vocab_size, output_length)\n",
    "attnisallyouneed_output = attnisallyouneed_embedding(vectorized_words)\n",
    "print(\"Output from my_embedded_layer: \", attnisallyouneed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00d8e9",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa1a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf2f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "#from multihead_attention import MultiHeadAttention\n",
    "#from positional_encoding import PositionEmbeddingFixedWeights\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "306c2e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.8561983  -1.0161369  -0.87670326 ...  0.71662605 -3.096138\n",
      "   -1.5791796 ]\n",
      "  [ 0.7081178  -1.6355238  -0.76292163 ...  0.20768908 -2.3993893\n",
      "   -1.0607417 ]\n",
      "  [ 0.8214517  -1.295295   -1.1473514  ... -0.73695964 -2.532823\n",
      "   -0.19046926]\n",
      "  [ 1.2936126  -1.2619262  -0.49072123 ... -0.11670566 -3.1790476\n",
      "    0.41199088]\n",
      "  [ 0.4483779  -1.0319082  -0.9776908  ...  1.1574054  -1.8842409\n",
      "   -1.3027635 ]]\n",
      "\n",
      " [[ 0.6870506  -1.5420032  -1.4347029  ...  0.6280665  -3.2174065\n",
      "   -0.2582354 ]\n",
      "  [ 0.67354596 -1.6792161  -0.3496355  ... -0.23691629 -1.511878\n",
      "   -1.1764946 ]\n",
      "  [ 1.0454015  -1.855876   -0.6921912  ...  0.5484423  -2.0834134\n",
      "    0.02199638]\n",
      "  [ 1.0398946  -0.6038212  -0.5984833  ...  0.2828291  -3.1353786\n",
      "   -0.3298112 ]\n",
      "  [-0.17609327 -1.614489   -1.1670898  ...  1.0516473  -3.1383228\n",
      "   -0.5770777 ]]\n",
      "\n",
      " [[ 0.5060721  -0.19114569 -0.94867253 ... -1.0266831  -3.3771384\n",
      "   -0.55645734]\n",
      "  [ 0.74764484 -0.886733   -1.0391564  ...  0.46551928 -3.167173\n",
      "   -1.2266376 ]\n",
      "  [ 0.9790301  -0.914185   -0.68624705 ... -0.07900256 -4.1673427\n",
      "   -1.4616426 ]\n",
      "  [ 1.1096314  -1.1158149  -1.5170642  ...  0.5561078  -2.9673223\n",
      "   -1.4473224 ]\n",
      "  [ 0.89327043 -1.4841609  -0.78320056 ... -0.04226022 -2.9535477\n",
      "   -1.4478495 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.3873799  -0.97673714 -0.7847227  ...  0.02999882 -2.885639\n",
      "   -0.86503154]\n",
      "  [ 0.5802444  -0.31310052 -0.76583296 ... -0.185948   -1.8961765\n",
      "   -0.18215558]\n",
      "  [ 0.23797958 -0.6455281  -1.3385923  ... -0.8789892  -2.879139\n",
      "   -0.10258996]\n",
      "  [ 0.64505833 -1.6994114  -0.85475373 ...  0.29686013 -2.2074842\n",
      "   -0.71248764]\n",
      "  [ 0.32767066 -1.1238005  -0.9419765  ...  0.08311326 -2.91962\n",
      "    0.07866223]]\n",
      "\n",
      " [[ 0.45772007 -0.24556322 -1.1844293  ... -0.16802043 -2.6990647\n",
      "   -0.22055706]\n",
      "  [ 1.1532925  -0.02207606 -1.4415792  ... -0.5110553  -2.9783373\n",
      "   -1.1190127 ]\n",
      "  [ 0.81710255 -1.2970409  -0.933987   ...  0.5963455  -2.2104921\n",
      "   -0.81280696]\n",
      "  [ 0.58433384 -1.0293229  -1.5259829  ... -0.39870107 -3.6421673\n",
      "   -1.1042923 ]\n",
      "  [ 0.4311922  -1.0940164  -1.1612736  ... -0.18210356 -3.101335\n",
      "   -1.3828124 ]]\n",
      "\n",
      " [[ 0.69099176 -1.3208513  -1.9140668  ... -0.47993273 -3.2300537\n",
      "   -0.73836476]\n",
      "  [ 1.0996757  -1.4097433  -0.6259195  ...  0.14300187 -3.1019397\n",
      "   -0.5957989 ]\n",
      "  [ 1.4844127  -0.95702934 -1.4962195  ...  0.19960865 -3.419769\n",
      "    0.787573  ]\n",
      "  [ 0.5955     -1.1011374  -1.2613982  ... -0.38960803 -3.1813192\n",
      "   -0.3483881 ]\n",
      "  [ 1.6809117  -0.6337149  -1.2827351  ...  0.03455511 -3.221347\n",
      "   -0.20583868]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43539a3",
   "metadata": {},
   "source": [
    "## decoder\n",
    "\n",
    "receives the **output of the encoder** + **decoder output at the previous time step** to generate an output sequence\n",
    "\n",
    "the **target sequence** (instead of the input sequence) that is embedded and augmented with positional information before being supplied to the decoder\n",
    "\n",
    "the **second multi-head attention block receives the encoder output** in the form of **keys and values** and the **normalized output** of the first decoder attention block as the **queries**.\n",
    "\n",
    "- encoder output = keys, values\n",
    "- normalized output of decoder first layer = queries\n",
    "\n",
    "decoder class receives h, d_k, d_v, d_model, d_ff and rate\n",
    "\n",
    "- in multihead attention 1 --> h=x, d_k=x, d_v=x, d_model=lookahead_mask\n",
    "- in multihead attention 2 --> h=addnorm_output1, d_k=encoder_output, d_v=encoder_output, d_model=padding_mask\n",
    "- in fully connected layer (feed forward) --> addnorm_output2\n",
    "-- addnorm_output2 --> addnorm_output1, multihead_output2\n",
    "-- addnorm_output1 --> add_norm1(x, multihead_output1)\n",
    "-- multihead_output2 --> multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "\n",
    "\n",
    "- h: 8  # Number of self-attention heads\n",
    "- d_k: 64  # Dimensionality of the linearly projected queries and keys\n",
    "- d_v: 64  # Dimensionality of the linearly projected values\n",
    "- d_ff: 2048  # Dimensionality of the inner fully connected layer\n",
    "- d_model: 512  # Dimensionality of the model sub-layers' outputs\n",
    "- n: 6  # Number of layers in the encoder stack\n",
    "\n",
    "\n",
    "- h: number of self attention heads\n",
    "- d_k: keys dim\n",
    "- d_v: values dim\n",
    "- d_model: model sub layers dim\n",
    "- d_ff: feed forward layer dim\n",
    "- rate: percentage of dropout (based on Vaswani et al. implementation of regularization before the layer norm step)\n",
    "\n",
    "\n",
    "- x: positional encoded tokens?\n",
    "- x = layer(x, encoder_output, lookahead_mask, padding_mask, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bbf8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dropout\n",
    "#from multihead_attention import MultiHeadAttention\n",
    "#from positional_encoding import PositionEmbeddingFixedWeights\n",
    "#from encoder import AddNormalization, FeedForward\n",
    " \n",
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)\n",
    "\n",
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbe293c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 2.0181537  -0.18450202  0.34749758 ... -0.2207683  -0.7662199\n",
      "    0.2995795 ]\n",
      "  [ 2.1014647  -0.20337886  0.38947013 ... -0.20284028 -0.8162828\n",
      "    0.28013954]\n",
      "  [ 2.0939398  -0.28071314  0.3640463  ... -0.17149572 -0.8658056\n",
      "    0.26109493]\n",
      "  [ 1.9819334  -0.32749534  0.2737773  ... -0.16544385 -0.8999605\n",
      "    0.26172018]\n",
      "  [ 1.8569611  -0.2664547   0.19059129 ... -0.18338226 -0.9083579\n",
      "    0.29545364]]\n",
      "\n",
      " [[ 2.300218   -0.11143223  0.4402657  ...  0.00888467 -0.7076979\n",
      "    0.36083898]\n",
      "  [ 2.387668   -0.12874289  0.49899402 ...  0.0503676  -0.75052845\n",
      "    0.36352432]\n",
      "  [ 2.3586142  -0.21971935  0.49038532 ...  0.09048974 -0.7754757\n",
      "    0.34675524]\n",
      "  [ 2.250057   -0.28364265  0.40738577 ...  0.09454537 -0.7956464\n",
      "    0.3361612 ]\n",
      "  [ 2.1308353  -0.24297541  0.30365276 ...  0.08399648 -0.7956701\n",
      "    0.3505138 ]]\n",
      "\n",
      " [[ 2.1028833  -0.14509012  0.2654457  ...  0.289588   -0.6898147\n",
      "    0.27826425]\n",
      "  [ 2.1884987  -0.16292253  0.3145886  ...  0.31782493 -0.7293498\n",
      "    0.26700634]\n",
      "  [ 2.1741867  -0.24661571  0.2862261  ...  0.35206234 -0.76337606\n",
      "    0.26111087]\n",
      "  [ 2.068547   -0.29829445  0.1981423  ...  0.3623056  -0.78288996\n",
      "    0.2707159 ]\n",
      "  [ 1.9523108  -0.24551792  0.10813542 ...  0.35334232 -0.7893677\n",
      "    0.2948166 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.2261577  -0.04640144  0.12978512 ... -0.23367976 -0.9570186\n",
      "    0.37922612]\n",
      "  [ 2.3146334  -0.0526633   0.18083243 ... -0.21372026 -0.9913112\n",
      "    0.3724576 ]\n",
      "  [ 2.2938864  -0.11116876  0.15857059 ... -0.17923187 -1.0341092\n",
      "    0.35374227]\n",
      "  [ 2.183798   -0.14570147  0.07994733 ... -0.15420997 -1.0589781\n",
      "    0.3470878 ]\n",
      "  [ 2.0624297  -0.09062686 -0.00674409 ... -0.15999112 -1.0547365\n",
      "    0.376916  ]]\n",
      "\n",
      " [[ 2.146523   -0.4106642   0.33936012 ...  0.0985149  -0.64147854\n",
      "    0.5995874 ]\n",
      "  [ 2.2316356  -0.42923278  0.39498085 ...  0.1096305  -0.6804211\n",
      "    0.5617458 ]\n",
      "  [ 2.2074885  -0.5117427   0.38768125 ...  0.1231982  -0.698016\n",
      "    0.52982837]\n",
      "  [ 2.098954   -0.5574886   0.30310547 ...  0.1404067  -0.7046604\n",
      "    0.52053404]\n",
      "  [ 1.9965696  -0.49742737  0.20064117 ...  0.13635705 -0.6990684\n",
      "    0.5661568 ]]\n",
      "\n",
      " [[ 2.1816812  -0.17574492  0.6087741  ... -0.08770858 -0.7720488\n",
      "    0.47223368]\n",
      "  [ 2.2626486  -0.1781441   0.66111267 ... -0.04714738 -0.80327874\n",
      "    0.45688996]\n",
      "  [ 2.2385237  -0.2453075   0.62739325 ... -0.02712529 -0.83177507\n",
      "    0.4420559 ]\n",
      "  [ 2.1448455  -0.28948736  0.5318943  ... -0.02539473 -0.8541051\n",
      "    0.44507566]\n",
      "  [ 2.0415277  -0.23560408  0.43946123 ... -0.03276727 -0.8660592\n",
      "    0.46141413]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the decoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(decoder(input_seq, enc_output, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e006ef",
   "metadata": {},
   "source": [
    "## encoder - decoder + masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f533eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from encoder import Encoder\n",
    "#from decoder import Decoder\n",
    "from tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
    "        super(TransformerModel, self).__init__(**kwargs)\n",
    "\n",
    "        # Set up the encoder\n",
    "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        # Set up the decoder\n",
    "        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
    "\n",
    "        # Define the final dense layer\n",
    "        self.model_last_layer = Dense(dec_vocab_size)\n",
    "\n",
    "    def padding_mask(self, input):\n",
    "        # Create mask which marks the zero padding values in the input by a 1.0\n",
    "        mask = math.equal(input, 0)\n",
    "        mask = cast(mask, float32)\n",
    "\n",
    "        # The shape of the mask should be broadcastable to the shape\n",
    "        # of the attention weights that it will be masking later on\n",
    "        return mask[:, newaxis, newaxis, :]\n",
    "\n",
    "    def lookahead_mask(self, shape):\n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training):\n",
    "\n",
    "        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n",
    "        enc_padding_mask = self.padding_mask(encoder_input)\n",
    "\n",
    "        # Create and combine padding and look-ahead masks to be fed into the decoder\n",
    "        dec_in_padding_mask = self.padding_mask(decoder_input)\n",
    "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
    "        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
    "\n",
    "        # Feed the input into the encoder\n",
    "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    "\n",
    "        # Feed the encoder output into the decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n",
    "\n",
    "        # Pass the decoder output through a final dense layer\n",
    "        model_output = self.model_last_layer(decoder_output)\n",
    "\n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14951b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 5  # Maximum length of the input sequence\n",
    "dec_seq_length = 5  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b87c1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder layer\n",
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.build(input_shape=[None, sequence_length, d_model])\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.d_model))\n",
    "        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n",
    "\n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ea3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4eaac46",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 7 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#from encoder import EncoderLayer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#from decoder import DecoderLayer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m encoder\u001b[38;5;241m.\u001b[39mbuild_graph()\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m      7\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 7 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "#from encoder import EncoderLayer\n",
    "#from decoder import DecoderLayer\n",
    "\n",
    "encoder = EncoderLayer(enc_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "encoder.build_graph().summary()\n",
    "\n",
    "decoder = DecoderLayer(dec_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate)\n",
    "decoder.build_graph().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc70833",
   "metadata": {},
   "source": [
    "## training transformer with actual sentences\n",
    "\n",
    "https://machinelearningmastery.com/training-the-transformer-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4059d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
